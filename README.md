I've developed an efficient discrimination algorithm for black-boxes that's nearly linear.

Virtually every industry that uses predictive policies or generates opportunities is susceptible to unfairness, even in unexpected cases like a product promotion or discount given to only select customers. Machine learning can have legal and ethical consequences if their outputs lead to demographic decision imbalances. Industries use pre-trained ML models to feed input data to produce an output – a decision for business actions. These models are composed of learned parameters that affect the probability of a decision. Decision bias can have multiple causes, such as if a model is trained on data containing minorities, ‘representation disparity’ can occur for minority groups. [5] Testing the fairness of these parameters in model fairness testing (MFT) is typically done without inspecting the parameters themselves. This makes the models a ‘black box,’ meaning that testing them entails giving them input and observing the output, without knowing how or why the model came to its output. Li et al. states model fairness emphasizes that “similar individuals should be treated similarly” [1].

![Model Output](assets/Local_Neighbor_Pert.png)
![Model Output](assets/Dist_Maintinance.png)
